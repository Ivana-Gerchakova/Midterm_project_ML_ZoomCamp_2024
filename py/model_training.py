# -*- coding: utf-8 -*-
"""Project 25.11.24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ncPLUke6sM0GPQs_DRLpUcsbXdosQZMr

### MIDTERM PROJECT

### LIBRARIES :
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import xgboost as xgb
from prettytable import PrettyTable
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor
from sklearn.ensemble import GradientBoostingRegressor

"""### READ THE DATASET"""

df = pd.read_csv('New York City Airbnb.csv')
df.head()

"""### TRAIN/VALIDATION/TEST SPLIT"""

df_temp, df_test = train_test_split(df, test_size=0.2, random_state=1)
df_train, df_val = train_test_split(df_temp, test_size=0.25, random_state=1)

df_train.reset_index(drop=True, inplace=True)
df_val.reset_index(drop=True, inplace=True)
df_test.reset_index(drop=True, inplace=True)

target_train = df_train['price'].values
target_validation = df_val['price'].values
target_test = df_test['price'].values

df_train.drop(columns=['price'], inplace=True)
df_val.drop(columns=['price'], inplace=True)
df_test.drop(columns=['price'], inplace=True)

"""### DICTVECTORIZER"""

train_records = df_train.to_dict(orient='records')
vectorizer = DictVectorizer(sparse=True)
X_train_vectorized = vectorizer.fit_transform(train_records)

validation_records = df_val.to_dict(orient='records')
X_validation_vectorized = vectorizer.transform(validation_records)

test_records = df_test.to_dict(orient='records')
X_test_vectorized = vectorizer.transform(test_records)

print(f'Shape of vectorized training data: {X_train_vectorized.shape}')
print(f'Shape of vectorized validation data: {X_validation_vectorized.shape}')
print(f'Shape of vectorized test data: {X_test_vectorized.shape}')

"""### FEATURE IMPORTANCE"""

model = DecisionTreeRegressor(max_depth=1, random_state=1)
model.fit(X_train_vectorized, target_train)

feature_importance = model.feature_importances_
important_feature_index = feature_importance.argmax()
important_feature = vectorizer.feature_names_[important_feature_index]

print(f'The feature used for splitting the data is: {important_feature}\n')

scaler = StandardScaler(with_mean=False)
X_train_scaled = scaler.fit_transform(X_train_vectorized)
X_validation_scaled = scaler.transform(X_validation_vectorized)

"""### MODEL TRAINING WITH ONE MODEL, NO PARAMETER TUNING."""

linear_model = LinearRegression()
linear_model.fit(X_train_scaled, target_train)

"""Linear Regression Model meaning it explains only 16% of the variance in the target variable. Its show struggles with variance, likely due to the model's simplicity in capturing complex relationships.

### TRAINED MULTIPLE MODELS (Desision tree, RandomForestRegressor, XGBRegressor) WITH PARAMETER TUNING
"""

dt_params = {'max_depth': [1, 5, 10, None], 'min_samples_split': [2, 10, 20]}
dt_grid = GridSearchCV(DecisionTreeRegressor(random_state=1), dt_params, cv=3)
dt_grid.fit(X_train_vectorized, target_train)
best_dt_model = dt_grid.best_estimator_

"""Decision Tree Model meaning it explains very little of the variance in the data. This model is less accurate, likely due to overfitting on certain patterns and lacking generalization."""

rf_params = {'n_estimators': [10, 20, 50], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5]}
rf_grid = GridSearchCV(RandomForestRegressor(random_state=1, n_jobs=-1), rf_params, cv=2)
rf_grid.fit(X_train_vectorized, target_train)
best_rf_model = rf_grid.best_estimator_

"""Random Forest Model performs slightly better with an R², indicating moderate explanatory power and variance handling. It show improved accuracy due to ensemble learning, which reduces overfitting."""

xgb_model = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=1)
xgb_model.fit(X_train_vectorized, target_train)

"""XGBoost Model has the best R² score, RMSE, and a low MAE, among the regression models, indicating it captures patterns slightly better. This is due to its boosted ensemble technique, which corrects errors iteratively and enhances predictive performance.

#### NEW FEATURES AND MODELS
"""

df['latitude_longitude'] = df['latitude'] * df['longitude']
df['reviews_per_month_ratio'] = df['number_of_reviews'] / (df['minimum_nights'] + 1)

"""### GRADIENT BOOSTING MODEL WITH TUNING"""

gb_params = {'n_estimators': [50, 100], 'max_depth': [3, 5], 'learning_rate': [0.01, 0.1]}
gb_grid = GridSearchCV(GradientBoostingRegressor(random_state=1), gb_params, cv=3, scoring='r2')
gb_grid.fit(X_train_vectorized, target_train)
best_gb_model = gb_grid.best_estimator_

"""Gradient Boosting Model shows performance similar to the Decision Tree, with an R², suggesting limited accuracy. This model show that it also struggled to generalize well."""

### TRAINING MULTIPLE VARIATIONS OF NEURAL NETWORKS WITH TUNED PARAMETERS"""

def create_nn_model(dropout_rate=0.2, learning_rate=0.001, layer_size=64, additional_layers=False):
    model = Sequential()
    model.add(Dense(layer_size, activation='relu', input_shape=(X_train_vectorized.shape[1],)))
    model.add(Dropout(dropout_rate))

    if additional_layers:
        model.add(Dense(layer_size // 2, activation='relu'))
        model.add(Dropout(dropout_rate))

    model.add(Dense(1))
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')
    return model

variations = [
    {'dropout_rate': 0.2, 'learning_rate': 0.001, 'layer_size': 64, 'additional_layers': False},
    {'dropout_rate': 0.3, 'learning_rate': 0.001, 'layer_size': 64, 'additional_layers': True},
    {'dropout_rate': 0.2, 'learning_rate': 0.0005, 'layer_size': 128, 'additional_layers': True},
    {'dropout_rate': 0.4, 'learning_rate': 0.0001, 'layer_size': 32, 'additional_layers': False},
]

trained_models = []
nn_predictions = []

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

for i, params in enumerate(variations):
    print(f'\nTraining Neural Network Variation {i+1} with params: {params}')
    model_nn = create_nn_model(**params)

    model_nn.fit(
        X_train_vectorized,
        target_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_validation_vectorized, target_validation),
        callbacks=[early_stopping],
        verbose=1
    )

    trained_models.append(model_nn)
    nn_val_predictions = model_nn.predict(X_validation_vectorized).flatten()
    nn_predictions.append(nn_val_predictions)
    print(f'Neural Network Model Variation {i+1} completed.')

"""
Neural networks showed negative R² values across all variations, indicating that they performed worse than a baseline model. This suggests that the networks struggled with high variance and failed to effectively capture meaningful relationships in the data.
"""

# The RMSE values were generally higher for neural networks compared to traditional regression models, further emphasizing their lower accuracy in this context.
# Key Insight: These neural network models may require further hyperparameter tuning, feature engineering, or alternative architectures to match or surpass the performance of simpler regression models. This highlights that simpler models can sometimes outperform more complex ones, especially when working with structured or limited datasets.











